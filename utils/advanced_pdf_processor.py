import os
import io
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime
import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import pdfplumber
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

class AdvancedPDFProcessor:
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200, debug: bool = False):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.debug = debug
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""],
            length_function=len
        )
        
        if debug:
            self.debug_dir = Path("debug_output")
            self.debug_dir.mkdir(exist_ok=True)
    
    def extract_with_pymupdf(self, pdf_path: str) -> List[Document]:
        """PyMuPDF ile metin Ã§Ä±karma"""
        documents = []
        pdf_document = fitz.open(pdf_path)
        
        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            text = page.get_text()
            
            documents.append(Document(
                page_content=text,
                metadata={
                    "source": os.path.basename(pdf_path),
                    "page": page_num + 1,
                    "extraction_method": "pymupdf"
                }
            ))
        
        pdf_document.close()
        return documents
    
    def extract_with_pdfplumber(self, pdf_path: str) -> List[Document]:
        """pdfplumber ile metin Ã§Ä±karma (tablo desteÄŸi)"""
        documents = []
        
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                # Normal metin Ã§Ä±karma
                text = page.extract_text() or ""
                
                # Tablo varsa tablolarÄ± da Ã§Ä±kar
                tables = page.extract_tables()
                if tables:
                    table_text = "\n\n=== TABLOLAR ===\n"
                    for table_num, table in enumerate(tables):
                        table_text += f"\nTablo {table_num + 1}:\n"
                        for row in table:
                            if row:
                                table_text += " | ".join([cell or "" for cell in row]) + "\n"
                    text += table_text
                
                documents.append(Document(
                    page_content=text,
                    metadata={
                        "source": os.path.basename(pdf_path),
                        "page": page_num + 1,
                        "extraction_method": "pdfplumber",
                        "has_tables": len(tables) > 0
                    }
                ))
        
        return documents
    
    def extract_with_ocr(self, pdf_path: str) -> List[Document]:
        """OCR ile metin Ã§Ä±karma (gÃ¶rsel PDF'ler iÃ§in)"""
        documents = []
        pdf_document = fitz.open(pdf_path)
        
        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            
            # SayfayÄ± gÃ¶rÃ¼ntÃ¼ye Ã§evir
            mat = fitz.Matrix(2.0, 2.0)  # YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼k iÃ§in
            pix = page.get_pixmap(matrix=mat)
            img_data = pix.tobytes("png")
            
            # PIL Image'a Ã§evir
            image = Image.open(io.BytesIO(img_data))
            
            # OCR uygula
            try:
                text = pytesseract.image_to_string(image, lang='tur')  # TÃ¼rkÃ§e OCR
            except Exception as e:
                print(f"OCR hatasÄ± sayfa {page_num + 1}: {e}")
                text = ""
            
            documents.append(Document(
                page_content=text,
                metadata={
                    "source": os.path.basename(pdf_path),
                    "page": page_num + 1,
                    "extraction_method": "ocr"
                }
            ))
        
        pdf_document.close()
        return documents
    
    def combine_extractions(self, extractions: Dict[str, List[Document]]) -> List[Document]:
        """FarklÄ± yÃ¶ntemlerden Ã§Ä±karÄ±lan metinleri birleÅŸtir"""
        combined_docs = []
        
        # Sayfa sayÄ±sÄ±nÄ± tespit et
        max_pages = max(len(docs) for docs in extractions.values())
        
        for page_num in range(max_pages):
            combined_text = ""
            metadata = {
                "source": "",
                "page": page_num + 1,
                "extraction_methods": []
            }
            
            # Her yÃ¶ntemden o sayfanÄ±n metnini al
            for method, docs in extractions.items():
                if page_num < len(docs):
                    doc = docs[page_num]
                    if doc.page_content.strip():
                        combined_text += f"\n=== {method.upper()} ===\n"
                        combined_text += doc.page_content + "\n"
                        metadata["extraction_methods"].append(method)
                        if not metadata["source"]:
                            metadata["source"] = doc.metadata.get("source", "")
            
            # En iyi metni seÃ§ (en uzun olanÄ±)
            best_text = ""
            best_method = ""
            
            for method, docs in extractions.items():
                if page_num < len(docs) and docs[page_num].page_content.strip():
                    if len(docs[page_num].page_content) > len(best_text):
                        best_text = docs[page_num].page_content
                        best_method = method
            
            combined_docs.append(Document(
                page_content=best_text,
                metadata={
                    "source": metadata["source"],
                    "page": page_num + 1,
                    "best_method": best_method,
                    "all_methods": ", ".join(metadata["extraction_methods"]),  # Liste yerine string
                    "method_count": len(metadata["extraction_methods"])  # Liste uzunluÄŸu
                }
            ))
        
        return combined_docs
    
    def save_extracted_text(self, combined_docs: List[Document], pdf_name: str):
        """Advanced PDF'ten Ã§Ä±karÄ±lan ham metni txt dosyasÄ±na kaydet"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{pdf_name}_{timestamp}_advanced_extracted_text.txt"
        filepath = self.debug_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"PDF: {pdf_name}\n")
            f.write(f"Ã‡Ä±karÄ±lma Tarihi: {datetime.now()}\n")
            f.write(f"Toplam Sayfa: {len(combined_docs)}\n")
            f.write(f"Ä°ÅŸleme Modu: ADVANCED (Ã‡oklu YÃ¶ntem)\n")
            f.write("="*80 + "\n\n")
            
            for i, doc in enumerate(combined_docs):
                page_num = doc.metadata.get('page', i+1)
                best_method = doc.metadata.get('best_method', 'unknown')
                all_methods = doc.metadata.get('all_methods', '')
                
                f.write(f"SAYFA {page_num}:\n")
                f.write(f"En Ä°yi YÃ¶ntem: {best_method}\n")
                f.write(f"KullanÄ±lan TÃ¼m YÃ¶ntemler: {all_methods}\n")
                f.write("-" * 40 + "\n")
                f.write(doc.page_content)
                f.write("\n\n" + "="*80 + "\n\n")
        
        print(f"Advanced ham metin kaydedildi: {filepath}")
        return filepath
    
    def save_chunked_text(self, chunks: List[Document], pdf_name: str):
        """Advanced PDF'ten oluÅŸturulan parÃ§alarÄ± txt dosyasÄ±na kaydet"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{pdf_name}_{timestamp}_advanced_chunks.txt"
        filepath = self.debug_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"PDF: {pdf_name}\n")
            f.write(f"ParÃ§alama Tarihi: {datetime.now()}\n")
            f.write(f"Toplam ParÃ§a: {len(chunks)}\n")
            f.write(f"Ä°ÅŸleme Modu: ADVANCED (Ã‡oklu YÃ¶ntem)\n")
            f.write("="*80 + "\n\n")
            
            for i, chunk in enumerate(chunks):
                f.write(f"PARÃ‡A {i+1}:\n")
                f.write(f"Kaynak: {chunk.metadata.get('source', 'Bilinmeyen')}\n")
                f.write(f"Sayfa: {chunk.metadata.get('page', 'Bilinmeyen')}\n")
                f.write(f"En Ä°yi YÃ¶ntem: {chunk.metadata.get('best_method', 'Bilinmeyen')}\n")
                f.write(f"KullanÄ±lan YÃ¶ntemler: {chunk.metadata.get('all_methods', 'Bilinmeyen')}\n")
                f.write(f"Karakter SayÄ±sÄ±: {len(chunk.page_content)}\n")
                f.write("-" * 40 + "\n")
                f.write(chunk.page_content)
                f.write("\n\n" + "="*80 + "\n\n")
        
        print(f"Advanced parÃ§alanmÄ±ÅŸ metin kaydedildi: {filepath}")
        return filepath

    def save_comparison_debug(self, extractions: Dict[str, List[Document]], pdf_name: str):
        """FarklÄ± yÃ¶ntemlerin karÅŸÄ±laÅŸtÄ±rmasÄ±nÄ± kaydet"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{pdf_name}_{timestamp}_extraction_comparison.txt"
        filepath = self.debug_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"PDF Ã‡IKARMA YÃ–NTEMLERÄ° KARÅILAÅTIRMASI\n")
            f.write(f"PDF: {pdf_name}\n")
            f.write(f"Tarih: {datetime.now()}\n")
            f.write("="*80 + "\n\n")
            
            # Genel istatistikler
            f.write("GENEL Ä°STATÄ°STÄ°KLER:\n")
            for method, docs in extractions.items():
                total_chars = sum(len(doc.page_content) for doc in docs)
                f.write(f"â€¢ {method.upper()}: {len(docs)} sayfa, {total_chars:,} karakter\n")
            f.write("\n" + "-"*60 + "\n\n")
            
            # Her sayfa iÃ§in karÅŸÄ±laÅŸtÄ±rma
            max_pages = max(len(docs) for docs in extractions.values())
            
            for page_num in range(max_pages):
                f.write(f"SAYFA {page_num + 1}:\n")
                f.write("-" * 60 + "\n")
                
                for method, docs in extractions.items():
                    if page_num < len(docs):
                        text = docs[page_num].page_content
                        f.write(f"\n{method.upper()} ({len(text)} karakter):\n")
                        f.write(text[:500] + ("..." if len(text) > 500 else "") + "\n")
                
                f.write("\n" + "="*80 + "\n\n")
        
        print(f"KarÅŸÄ±laÅŸtÄ±rma raporu kaydedildi: {filepath}")
        return filepath
    
    def process_pdf(self, pdf_path: str) -> List[Document]:
        """PDF'i tÃ¼m yÃ¶ntemlerle iÅŸle ve en iyi sonucu dÃ¶ndÃ¼r"""
        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
        
        if self.debug:
            print(f"ğŸ”„ {pdf_name} iÅŸleniyor - Ã§oklu yÃ¶ntem kullanÄ±lÄ±yor...")
        
        # TÃ¼m yÃ¶ntemleri dene
        extractions = {}
        
        try:
            # PyMuPDF (hÄ±zlÄ±, temel)
            extractions["pymupdf"] = self.extract_with_pymupdf(pdf_path)
            if self.debug:
                print("âœ“ PyMuPDF tamamlandÄ±")
        except Exception as e:
            print(f"âš  PyMuPDF hatasÄ±: {e}")
        
        try:
            # pdfplumber (tablo desteÄŸi)
            extractions["pdfplumber"] = self.extract_with_pdfplumber(pdf_path)
            if self.debug:
                print("âœ“ pdfplumber tamamlandÄ±")
        except Exception as e:
            print(f"âš  pdfplumber hatasÄ±: {e}")
        
        try:
            # OCR (gÃ¶rsel PDF'ler iÃ§in)
            extractions["ocr"] = self.extract_with_ocr(pdf_path)
            if self.debug:
                print("âœ“ OCR tamamlandÄ±")
        except Exception as e:
            print(f"âš  OCR hatasÄ±: {e}")
        
        if not extractions:
            raise Exception("HiÃ§bir Ã§Ä±karma yÃ¶ntemi baÅŸarÄ±lÄ± olmadÄ±!")
        
        # Debug: KarÅŸÄ±laÅŸtÄ±rma kaydet
        if self.debug:
            self.save_comparison_debug(extractions, pdf_name)
        
        # En iyi sonuÃ§larÄ± birleÅŸtir
        combined_docs = self.combine_extractions(extractions)
        
        # Debug: Ham metni kaydet (birleÅŸtirilmiÅŸ)
        if self.debug:
            self.save_extracted_text(combined_docs, pdf_name)
            print(f"âœ“ Ham metin kaydedildi: {len(combined_docs)} sayfa")
        
        # Metni parÃ§alara ayÄ±r
        chunks = self.text_splitter.split_documents(combined_docs)
        
        # Metadata gÃ¼ncelle
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                "chunk_id": i,
                "processing_method": "advanced_multi_extraction"
            })
        
        # Debug: ParÃ§alanmÄ±ÅŸ metni kaydet
        if self.debug:
            self.save_chunked_text(chunks, pdf_name)
            print(f"âœ“ ParÃ§alanmÄ±ÅŸ metin kaydedildi: {len(chunks)} parÃ§a")
        
        if self.debug:
            print(f"âœ“ {len(chunks)} parÃ§a oluÅŸturuldu")
            
            # Ä°statistikler
            total_chars = sum(len(chunk.page_content) for chunk in chunks)
            print(f"âœ“ Toplam karakter: {total_chars:,}")
            
            # En baÅŸarÄ±lÄ± yÃ¶ntemleri gÃ¶ster
            method_stats = {}
            for chunk in chunks:
                method = chunk.metadata.get("best_method", "unknown")
                method_stats[method] = method_stats.get(method, 0) + 1
            
            print("âœ“ YÃ¶ntem baÅŸarÄ± oranlarÄ±:")
            for method, count in method_stats.items():
                percentage = (count / len(chunks)) * 100
                print(f"  - {method}: {count} parÃ§a (%{percentage:.1f})")
            
            # Debug dosyalarÄ± Ã¶zeti
            print(f"\nğŸ“ Debug DosyalarÄ±:")
            print(f"  - {pdf_name}_*_advanced_extracted_text.txt (Ham metin)")
            print(f"  - {pdf_name}_*_advanced_chunks.txt (ParÃ§alar)")
            print(f"  - {pdf_name}_*_extraction_comparison.txt (YÃ¶ntem karÅŸÄ±laÅŸtÄ±rmasÄ±)")
        
        return chunks

# Gereksinimler iÃ§in yardÄ±mcÄ± kontrol fonksiyonu
def check_dependencies():
    """Gerekli kÃ¼tÃ¼phanelerin kurulu olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
    missing = []
    
    try:
        import fitz
    except ImportError:
        missing.append("PyMuPDF")
    
    try:
        import pdfplumber
    except ImportError:
        missing.append("pdfplumber")
    
    try:
        import pytesseract
        from PIL import Image
    except ImportError:
        missing.append("pytesseract ve/veya Pillow")
    
    if missing:
        print("âš  Eksik kÃ¼tÃ¼phaneler:")
        for lib in missing:
            print(f"  - {lib}")
        print("\nKurulum iÃ§in:")
        print("pip install PyMuPDF pdfplumber pytesseract Pillow")
        print("\nOCR iÃ§in Tesseract kurulumu gerekli:")
        print("- Windows: https://github.com/UB-Mannheim/tesseract/wiki")
        print("- macOS: brew install tesseract")
        print("- Linux: sudo apt install tesseract-ocr tesseract-ocr-tur")
        return False
    
    return True