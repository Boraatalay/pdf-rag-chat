from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory
import requests
import json

class RAGChain:
    def __init__(self, vectorstore, model_name: str, base_url: str):
        self.vectorstore = vectorstore
        self.model_name = model_name
        self.base_url = base_url
        self.temperature = 0.1
        
        # Memory ekleme - son 5 konuÅŸmayÄ± hatÄ±rlar
        self.memory = ConversationBufferWindowMemory(
            k=5,  # Son 5 soru-cevap Ã§iftini hatÄ±rla
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # GÃ¼ncellenmiÅŸ prompt - konuÅŸma geÃ§miÅŸi dahil
        self.prompt_template = """AÅŸaÄŸÄ±da verilen baÄŸlam, bir PDF belgesinden alÄ±nmÄ±ÅŸtÄ±r. 
Ã–nceki konuÅŸma geÃ§miÅŸini de dikkate alarak kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla.

âš ï¸ UyarÄ±lar:
- Ã–ncelikle PDF baÄŸlamÄ±ndaki bilgileri kullan
- Ã–nceki konuÅŸmalarda geÃ§en bilgilere referans verebilirsin
- Sadece baÄŸlamda geÃ§en ifadeleri kullan
- Cevap bulamazsan, "Bu sorunun cevabÄ± PDF iÃ§eriÄŸinde aÃ§Ä±kÃ§a belirtilmemiÅŸ." yaz

---

ğŸ“„ PDF BaÄŸlamÄ±:
{context}

ğŸ’¬ Ã–nceki KonuÅŸma:
{chat_history}

â“ Soru:
{question}

---

ğŸ“Œ Cevap:
"""
    
    def query(self, question: str) -> dict:
        """Soruyu yanÄ±tla - Direct Ollama API ile"""
        
        # Retriever ile dokÃ¼manlarÄ± al
        docs = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 15}
        ).get_relevant_documents(question)
        
        # Context oluÅŸtur
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # Memory'den chat history al
        chat_history = ""
        if hasattr(self.memory, 'chat_memory') and self.memory.chat_memory.messages:
            for msg in self.memory.chat_memory.messages[-6:]:  # Son 6 mesaj
                if hasattr(msg, 'content'):
                    role = "Ä°nsan" if msg.__class__.__name__ == "HumanMessage" else "Asistan"
                    chat_history += f"{role}: {msg.content}\n"
        
        # Final prompt oluÅŸtur
        final_prompt = self.prompt_template.format(
            context=context,
            chat_history=chat_history,
            question=question
        )
        
        # Direct Ollama API Ã§aÄŸrÄ±sÄ±
        response = requests.post(f'{self.base_url}/api/generate',
            json={
                "model": self.model_name,
                "prompt": final_prompt,
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "top_p": 0.9,
                    "top_k": 40
                }
            }
        )
        
        if response.status_code == 200:
            answer = response.json().get('response', 'Cevap alÄ±namadÄ±.')
        else:
            answer = f"API hatasÄ±: {response.status_code}"
        
        # Memory'ye ekle
        from langchain.schema import HumanMessage, AIMessage
        self.memory.chat_memory.add_user_message(question)
        self.memory.chat_memory.add_ai_message(answer)
        
        return {
            "answer": answer,
            "source_documents": docs
        }
    
    def clear_memory(self):
        """KonuÅŸma geÃ§miÅŸini temizle"""
        self.memory.clear()

    def update_temperature(self, temperature: float):
        """Temperature'Ä± gÃ¼ncelle"""
        self.memory.clear()
        self.temperature = temperature
        print(f"ğŸŒ¡ï¸ Temperature {temperature} olarak gÃ¼ncellendi!")
        print(f"ğŸ” Aktif Temperature: {self.temperature}")

    def get_current_temperature(self) -> float:
        """Mevcut temperature deÄŸerini dÃ¶ndÃ¼r"""
        return self.temperature
    
    def get_memory_summary(self):
        """Memory durumu hakkÄ±nda bilgi dÃ¶ndÃ¼r"""
        try:
            message_count = len(self.memory.chat_memory.messages)
            return f"HafÄ±zada {message_count//2} konuÅŸma var"
        except:
            return "Memory durumu alÄ±namadÄ±"