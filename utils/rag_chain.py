from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory

class RAGChain:
    def __init__(self, vectorstore, model_name: str, base_url: str):
        self.vectorstore = vectorstore
        
        # Memory ekleme - son 5 konuÅŸmayÄ± hatÄ±rlar
        self.memory = ConversationBufferWindowMemory(
            k=5,  # Son 5 soru-cevap Ã§iftini hatÄ±rla
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # GÃ¼ncellenmiÅŸ prompt - konuÅŸma geÃ§miÅŸi dahil
        self.prompt_template = """AÅŸaÄŸÄ±da verilen baÄŸlam, bir PDF belgesinden alÄ±nmÄ±ÅŸtÄ±r. 
Ã–nceki konuÅŸma geÃ§miÅŸini de dikkate alarak kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla.

âš ï¸ UyarÄ±lar:
- Ã–ncelikle PDF baÄŸlamÄ±ndaki bilgileri kullan
- Ã–nceki konuÅŸmalarda geÃ§en bilgilere referans verebilirsin
- Sadece baÄŸlamda geÃ§en ifadeleri kullan
- Cevap bulamazsan, "Bu sorunun cevabÄ± PDF iÃ§eriÄŸinde aÃ§Ä±kÃ§a belirtilmemiÅŸ." yaz

---

ğŸ“„ PDF BaÄŸlamÄ±:
{context}

ğŸ’¬ Ã–nceki KonuÅŸma:
{chat_history}

â“ Soru:
{question}

---

ğŸ“Œ Cevap:
"""
        
        self.PROMPT = PromptTemplate(
            template=self.prompt_template,
            input_variables=["context", "chat_history", "question"]
        )
        
        # Ollama LLM
        self.llm = Ollama(
            model=model_name,
            base_url=base_url,
            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
            temperature=0.1  # Biraz creativity iÃ§in
        )
        
        # ConversationalRetrievalChain kullan
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 15}
            ),
            memory=self.memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": self.PROMPT},
            verbose=False
        )
    
    def query(self, question: str) -> dict:
        """Soruyu yanÄ±tla ve kaynak belgeleri dÃ¶ndÃ¼r - Memory ile"""
        result = self.qa_chain.invoke({"question": question})
        return {
            "answer": result["answer"],
            "source_documents": result["source_documents"]
        }
    
    def clear_memory(self):
        """KonuÅŸma geÃ§miÅŸini temizle"""
        self.memory.clear()
    
    def get_memory_summary(self):
        """Memory durumu hakkÄ±nda bilgi dÃ¶ndÃ¼r"""
        try:
            message_count = len(self.memory.chat_memory.messages)
            return f"HafÄ±zada {message_count//2} konuÅŸma var"
        except:
            return "Memory durumu alÄ±namadÄ±"